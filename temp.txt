Awesome ‚Äî let‚Äôs consolidate **everything you've built so far** into a **complete, runnable FastAPI + Kafka + XGBoost real-time Bitcoin OHLC prediction system**.

---

## ‚úÖ Final Project Overview

You‚Äôve built a system that:

1. **Producer**: Gets BTC OHLC data from Binance every 1 min and sends it to Kafka
2. **Consumer + FastAPI**: Listens to Kafka, stores latest row in memory
3. **ML Model**: Trained on past OHLC to predict next OHLC values using XGBoost
4. **FastAPI endpoint**: Returns predictions in real-time from most recent message

---

## üìÅ Final Folder Structure

```
btc_predictor/
‚îú‚îÄ‚îÄ btc_producer.py            ‚Üê Gets OHLC every 1 min and sends to Kafka
‚îú‚îÄ‚îÄ btc_ohlc_data.csv          ‚Üê CSV storing past OHLC data (used to train)
‚îú‚îÄ‚îÄ model_trainer.py           ‚Üê Trains ML model and saves it as .pkl files
‚îú‚îÄ‚îÄ model_open.pkl             ‚Üê Trained model files
‚îú‚îÄ‚îÄ model_high.pkl
‚îú‚îÄ‚îÄ model_low.pkl
‚îú‚îÄ‚îÄ model_close.pkl
‚îú‚îÄ‚îÄ kafka_consumer.py          ‚Üê Kafka background listener (FastAPI reads from here)
‚îú‚îÄ‚îÄ main.py                    ‚Üê FastAPI app for live prediction
‚îî‚îÄ‚îÄ requirements.txt           ‚Üê All pip dependencies
```

---

## ‚úÖ STEP-BY-STEP EXECUTION

---

### üîß Step 0: Setup Kafka

Start **Zookeeper** and **Kafka broker** from your Kafka installation:

```bash
# Terminal 1
.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties

# Terminal 2
.\bin\windows\kafka-server-start.bat .\config\server.properties

# Terminal 3 (Create topic)
.\bin\windows\kafka-topics.bat --create --topic btc_ohlc --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
```

---

### ‚úÖ Step 1: Train ML Model (Only Once)

**File**: `model_trainer.py`

```python
# model_trainer.py
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
import joblib

df = pd.read_csv("btc_ohlc_data.csv", parse_dates=["timestamp"])
for col in ["open", "high", "low", "close"]:
    df[f"{col}_lag1"] = df[col].shift(1)
df = df.dropna()

X = df[["open_lag1", "high_lag1", "low_lag1", "close_lag1"]]
y_open = df["open"]
y_high = df["high"]
y_low = df["low"]
y_close = df["close"]

X_train, _, y_open_train, _ = train_test_split(X, y_open, shuffle=False, test_size=0.2)
_, _, y_high_train, _ = train_test_split(X, y_high, shuffle=False, test_size=0.2)
_, _, y_low_train, _ = train_test_split(X, y_low, shuffle=False, test_size=0.2)
_, _, y_close_train, _ = train_test_split(X, y_close, shuffle=False, test_size=0.2)

joblib.dump(XGBRegressor().fit(X_train, y_open_train), "model_open.pkl")
joblib.dump(XGBRegressor().fit(X_train, y_high_train), "model_high.pkl")
joblib.dump(XGBRegressor().fit(X_train, y_low_train), "model_low.pkl")
joblib.dump(XGBRegressor().fit(X_train, y_close_train), "model_close.pkl")

print("‚úÖ Models trained and saved")
```

**Run:**

```bash
python model_trainer.py
```

---

### ‚úÖ Step 2: Start Kafka Producer

**File**: `btc_producer.py`

```python
# btc_producer.py
import time, json, requests
from datetime import datetime
from confluent_kafka import Producer

TOPIC = "btc_ohlc"
BOOTSTRAP_SERVERS = "localhost:9092"
p = Producer({'bootstrap.servers': BOOTSTRAP_SERVERS})

def fetch_current_ohlc():
    url = "https://api.binance.us/api/v3/klines"
    params = {"symbol": "BTCUSDT", "interval": "1m", "limit": 1}
    data = requests.get(url, params=params).json()[0]
    return {
        "timestamp": datetime.fromtimestamp(data[0] / 1000).strftime("%Y-%m-%d %H:%M:%S"),
        "open": float(data[1]),
        "high": float(data[2]),
        "low": float(data[3]),
        "close": float(data[4])
    }

while True:
    try:
        msg = json.dumps(fetch_current_ohlc())
        p.produce(TOPIC, value=msg)
        p.flush()
        print("üì§ Sent OHLC to Kafka")
    except Exception as e:
        print("‚ùå Error:", e)
    time.sleep(60)
```

**Run in terminal**:

```bash
python btc_producer.py
```

---

### ‚úÖ Step 3: Kafka Consumer (Shared State for FastAPI)

**File**: `kafka_consumer.py`

```python
# kafka_consumer.py
import json
from confluent_kafka import Consumer
from threading import Thread

latest_row = {}

def start_kafka_listener():
    def loop():
        global latest_row
        c = Consumer({
            'bootstrap.servers': 'localhost:9092',
            'group.id': 'btc_fastapi',
            'auto.offset.reset': 'latest'
        })
        c.subscribe(["btc_ohlc"])
        print("üì• Kafka listener running...")

        while True:
            msg = c.poll(1.0)
            if msg is None or msg.error():
                continue
            latest_row = json.loads(msg.value().decode("utf-8"))

    Thread(target=loop, daemon=True).start()
```

---

### ‚úÖ Step 4: FastAPI App

**File**: `main.py`

```python
# main.py
from fastapi import FastAPI
import pandas as pd
import joblib
from kafka_consumer import latest_row, start_kafka_listener

app = FastAPI()
start_kafka_listener()

model_open = joblib.load("model_open.pkl")
model_high = joblib.load("model_high.pkl")
model_low = joblib.load("model_low.pkl")
model_close = joblib.load("model_close.pkl")

@app.get("/")
def root():
    return {"status": "Running", "try": "/predict"}

@app.get("/predict")
def predict():
    if not latest_row:
        return {"error": "No data received yet"}
    
    X = pd.DataFrame([{
        "open_lag1": latest_row["open"],
        "high_lag1": latest_row["high"],
        "low_lag1": latest_row["low"],
        "close_lag1": latest_row["close"]
    }])

    return {
        "predicted_open": float(model_open.predict(X)[0]),
        "predicted_high": float(model_high.predict(X)[0]),
        "predicted_low": float(model_low.predict(X)[0]),
        "predicted_close": float(model_close.predict(X)[0]),
    }
```

---

### ‚úÖ Step 5: Run FastAPI Server

```bash
uvicorn main:app --reload
```

Then open your browser at:

```
http://127.0.0.1:8000/predict
```

---

## ‚úÖ `requirements.txt`

```txt
pandas
xgboost
fastapi
uvicorn
confluent-kafka
joblib
requests
```

Install all:

```bash
pip install -r requirements.txt
```

---

## üîÅ Execution Order Summary

| Order | File               | Purpose                                          |
| ----- | ------------------ | ------------------------------------------------ |
| 0     | Kafka terminals    | Start Kafka & topic                              |
| 1     | `model_trainer.py` | Train ML models and save `.pkl` files            |
| 2     | `btc_producer.py`  | Fetch BTC data and stream to Kafka every 1 min   |
| 3     | `main.py`          | Start FastAPI which also launches Kafka consumer |
| 4     | Visit `/predict`   | Get real-time ML prediction                      |

---

Would you like to dockerize this whole setup or deploy the FastAPI app to a cloud platform like Render or AWS?
